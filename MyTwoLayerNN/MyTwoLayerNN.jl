module MyTwoLayerNN

using Random, Distributions


# Activation functions
Ïƒ(x) = max(0, x)
âˆ‚Ïƒ(x) = x < 0 ? 0 : 1

# Risk function
Rs(x, y) = sum(z -> z^2, x - y) / (2 * length(x))
âˆ‚Rs(x, y) = sum(x - y) / length(x)

# Creates structure to store the NN data
struct TwoLayerNN
    d::Integer                  # Number of input nodes
    m::Integer                  # Number of nodes in the hidden layer
    w::Matrix{Float64}          # Weights to hidden layer
    a::Vector{Float64}          # Weights from hidden layer
    b::Vector{Float64}          # Bias
    Î±::Float64                  # Scaling factor
end
TwoLayerNN(d::T, m::T, Î³, Î³â€²) where {T <: Integer} = begin
    # Set seed
    Random.seed!(123)

    # Parameters
    Î± = m^(Î³ + Î³â€²)
    Î²â‚= m^(-Î³â€²)
    Î²â‚‚= d^(-Î³â€²)

    # Initialize weights and biases
    w::Matrix{Float64} = rand(Normal(0, Î²â‚‚), m, d)
    a::Vector{Float64} = rand(Normal(0, Î²â‚), m)
    b::Vector{Float64} = rand(Normal(0, Î²â‚‚), m)

    # Create the NN
    TwoLayerNN(d, m, w, a, b, Î±)
end 

# Structure to store the training data 
struct TrainingData 
    n::Int32
    x::Vector{Vector{Float64}}
    y::Vector{Float64}
    learning_rate::Float64
    steps::Int32
end

# Calculate output of NN
function forward(nn::TwoLayerNN, x::Vector{T}) where {T <: Real}
    nn.a' * (Ïƒ.(nn.w * x .+ nn.b)) ./ nn.Î±
end
forward(nn::TwoLayerNN, x::T) where {T <: Real} = forward(nn, [x])
forward(nn::TwoLayerNN, x::Vector{Vector{T}}) where {T <: Real} = map(z -> forward(nn, z), x)
forward(nn::TwoLayerNN, x::AbstractRange{T}) where {T <: Real} = map(z -> forward(nn, z), x)

function forward!(nn::TwoLayerNN, x, inHL::Vector{T}, outHL::Vector{T}) where {T <: Real}
    inHL .= nn.w * x .+ nn.b
    outHL .= Ïƒ.(inHL)
    return nn.a' * outHL ./ nn.Î±    
end

# Trains the NN with the training data
function train!(nn::TwoLayerNN, trainData::TrainingData)
    # Create aliases for data 
    d = nn.d
    m = nn.m
    n = trainData.n
    steps = trainData.steps

    # Allocate memory for gradiants and values that go in the hidden layer and out
    gradData = (
        âˆ‡a = zeros(m), âˆ‡w = zeros(m, d), âˆ‡b = zeros(m), 
        inL = zeros(m), outL = zeros(m)
    )

    # Allocate memory for Adam optimization
    adamParms = (
        ma = zeros(m), mw = zeros(m, d), mb = zeros(m), 
        va = zeros(m), vw = zeros(m, d), vb = zeros(m), 
        Î²â‚˜ = 0.9, Î²áµ¥ = 0.999, Î²â‚š = [0.9, 0.999], Î· = trainData.learning_rate
    )

    # Gradient descent
    for s = 1:steps
        # Reset gradiants
        fill!(gradData.âˆ‡a, 0)
        fill!(gradData.âˆ‡w, 0)
        fill!(gradData.âˆ‡b, 0)

        # Sum the gradiant for all data points
        for i = 1:n
            updateGradiant!(i, nn, trainData, gradData)
        end

        # Apply adam
        applyAdam!(adamParms, nn, gradData)

    end
end

# Calculates the âˆ‡ of the NN with the ith data point and adds it to the total âˆ‡ 
function updateGradiant!(i, nn::TwoLayerNN, trainData::TrainingData, âˆ‡data)
    # Aliases 
    a = nn.a
    Î± = nn.Î±
    dataX = trainData.x
    dataY = trainData.y
    n = trainData.n
    inHL = âˆ‡data.inL
    outHL = âˆ‡data.outL
    âˆ‡a = âˆ‡data.âˆ‡a
    âˆ‡w = âˆ‡data.âˆ‡w
    âˆ‡b = âˆ‡data.âˆ‡b

    # Calculate the prediction of ith data point and store 
    # the values that went in and out the hidden layer
    predicted = forward!(nn, dataX[i], inHL, outHL)

    # Calculate the gradiants
    âˆ‚Riskâˆ‚p = (predicted - dataY[i]) / (Î± * n)

    # Calculate âˆ‡a
    @. âˆ‡a += âˆ‚Riskâˆ‚p * outHL

    # Calculate âˆ‡b
    @. âˆ‡w += âˆ‚Riskâˆ‚p * a * âˆ‚Ïƒ.(inHL) * dataX[i]'

    # Calculate âˆ‡b
    @. âˆ‡b += âˆ‚Riskâˆ‚p * a * âˆ‚Ïƒ.(inHL)
end

# Optimization method - used to update the âˆ‡
function applyAdam!(o, nn::TwoLayerNN, âˆ‡data)
    applyAdam!(o.ma, o.va, o.Î²â‚˜, o.Î²áµ¥, o.Î²â‚š, o.Î·, âˆ‡data.âˆ‡a, nn.a)
    applyAdam!(o.mw, o.vw, o.Î²â‚˜, o.Î²áµ¥, o.Î²â‚š, o.Î·, âˆ‡data.âˆ‡w, nn.w)
    applyAdam!(o.mb, o.vb, o.Î²â‚˜, o.Î²áµ¥, o.Î²â‚š, o.Î·, âˆ‡data.âˆ‡b, nn.b)

    o.Î²â‚š[1] *= o.Î²â‚˜
    o.Î²â‚š[2] *= o.Î²áµ¥
end

function applyAdam!(m, v, Î²â‚˜ :: Real, Î²áµ¥ :: Real, Î²â‚š, Î· :: Real, âˆ‡, w)
    @. m = Î²â‚˜ * m + (1 - Î²â‚˜) * âˆ‡
    @. v = Î²áµ¥ * v + (1 - Î²áµ¥) * âˆ‡ * âˆ‡
    @fastmath @. w -=  Î· * m / (1 - Î²â‚š[1]) / (âˆš(v / (1 - Î²â‚š[2])) + 1e-8)
end

# Creates a short summary of the NN
function summary(nn::TwoLayerNN)
    printstyled("Summary of neural network ðŸ§ : \n", color = :blue)
    println("d = $(nn.d), m = $(nn.m), Î± = $(nn.Î±)")
    
    w_max = max(nn.w ...)
    w_min = min(nn.w ...)
    w_avg = sum(nn.w) / length(nn.w)
    println("w: max = $w_max, min = $w_min, avg = $w_avg")

    a_max = max(nn.a ...)
    a_min = min(nn.a ...)
    a_avg = sum(nn.a) / length(nn.a)
    println("a: max = $a_max, min = $a_min, avg = $a_avg")

    b_max = max(nn.b ...)
    b_min = min(nn.b ...)
    b_avg = sum(nn.b) / length(nn.b)
    println("Bias: max = $b_max, min = $b_min, avg = $b_avg")
end

end;